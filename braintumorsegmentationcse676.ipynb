{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# First, Import all required Data,(MANDATORY)\n# import data by URL: https://www.kaggle.com/datasets/awsaf49/brats20-dataset-training-validation\n# import data by URL: https://www.kaggle.com/datasets/atulsingh1996/cse676-brats-semantic-segmentation\n\n#ONLY EDIT THIS SECTION - BEGIN\n\n# MODE can be 'TRAIN_FRESH' or 'EVALUATE'\n# To do fresh training followed by its evaluation, Set to TRAIN_FRESH\n# To only evaluate our pre-trained model, Set to EVALUATE\nMODE = 'EVALUATE'\n\n# This is the model architecture you want to train or test. It can only take the following values.\n#'simpleUNET', 'resNETUNET', 'WNET', 'UNETVAE', 'VGG19', 'InceptionV3', 'InceptionResNetV2'\nCURRENT_MODEL = 'WNET'\n\n\n#ONLY EDIT THIS SECTION - END","metadata":{"_uuid":"babd552b-3a36-4b0e-9953-15646c660b16","_cell_guid":"7b9bfafd-f6c6-44c8-bcaf-7315a2cfee04","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:42:32.059218Z","iopub.execute_input":"2022-05-23T01:42:32.059889Z","iopub.status.idle":"2022-05-23T01:42:32.064397Z","shell.execute_reply.started":"2022-05-23T01:42:32.059846Z","shell.execute_reply":"2022-05-23T01:42:32.063539Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"!jupyter --version","metadata":{"execution":{"iopub.status.busy":"2022-05-23T01:42:32.104736Z","iopub.execute_input":"2022-05-23T01:42:32.105047Z","iopub.status.idle":"2022-05-23T01:42:33.747785Z","shell.execute_reply.started":"2022-05-23T01:42:32.105014Z","shell.execute_reply":"2022-05-23T01:42:33.746852Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"!pip install segmentation-models\n!pip install nilearn","metadata":{"_uuid":"a52fce3f-2800-4728-b8ed-54252b833e33","_cell_guid":"53c9e470-f374-453d-a96a-b9aec8e40651","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:42:33.751156Z","iopub.execute_input":"2022-05-23T01:42:33.751690Z","iopub.status.idle":"2022-05-23T01:42:52.543619Z","shell.execute_reply.started":"2022-05-23T01:42:33.751650Z","shell.execute_reply":"2022-05-23T01:42:52.542656Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"import nilearn\nimport nibabel as nib\nimport nilearn.plotting as nlplt","metadata":{"_uuid":"f895285a-f4cf-4c50-9373-38b240856348","_cell_guid":"158abba1-a810-4c9a-99ee-70523c0e222e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:42:52.547177Z","iopub.execute_input":"2022-05-23T01:42:52.547425Z","iopub.status.idle":"2022-05-23T01:42:52.553401Z","shell.execute_reply.started":"2022-05-23T01:42:52.547399Z","shell.execute_reply":"2022-05-23T01:42:52.552406Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport glob\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import data\nfrom skimage.util import montage\nfrom skimage.transform import rotate\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport random\nimport pandas as pd\n\n# Segmentation Models\nimport segmentation_models as sm\n\n# Keras\nimport tensorflow as tf\nimport keras\nimport keras.backend as K\nfrom keras.callbacks import CSVLogger\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nnp.set_printoptions(precision=3, suppress=True)","metadata":{"_uuid":"a615d739-a04e-483a-a72e-de7348bf316e","_cell_guid":"479d014d-a22f-48ef-b74e-0b7044a0f279","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:42:52.555939Z","iopub.execute_input":"2022-05-23T01:42:52.556449Z","iopub.status.idle":"2022-05-23T01:42:52.567627Z","shell.execute_reply.started":"2022-05-23T01:42:52.556409Z","shell.execute_reply":"2022-05-23T01:42:52.566786Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"TRAINING_DATASET = \"../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/*\"\n\nIMAGE_DIR = glob.glob(TRAINING_DATASET)\nCSV_LIST = glob.glob(TRAINING_DATASET + 'csv')\nfor i in CSV_LIST:\n    IMAGE_DIR.remove(i)\n\nIMAGE_DIR.remove(TRAINING_DATASET.replace(\"*\", \"\") + 'BraTS20_Training_355')\nIMAGE_DIR = IMAGE_DIR[0:100]\n\nTRAIN_LIST, VAL_LIST = train_test_split(IMAGE_DIR, test_size=0.2)\nTRAIN_LIST, TEST_LIST = train_test_split(TRAIN_LIST, test_size=0.3)\n\nVIEW_IMG_IDX = random.randint(0, len(TRAIN_LIST) - 1)\nLIST_DATA = sorted(glob.glob(TRAIN_LIST[VIEW_IMG_IDX] + '/*'))\nVIEW_IMAGE = {\n    'flair': nib.load(LIST_DATA[0]).get_fdata(),\n    't1': nib.load(LIST_DATA[2]).get_fdata(),\n    't1ce': nib.load(LIST_DATA[3]).get_fdata(),\n    't2': nib.load(LIST_DATA[4]).get_fdata(),\n    'mask': nib.load(LIST_DATA[1]).get_fdata()\n}\n\nMODEL = {\n    'simpleUNET': 'simpleUNET.h5',\n    'resNETUNET': 'resnetUNET.h5',\n    'WNET': 'WNET.h5',\n    'UNETVAE': 'UNETVAE.h5',\n    'VGG19': 'VGG19.h5',\n    'InceptionV3': 'InceptionV3.h5',\n    'InceptionResNetV2': 'InceptionResNetV2.h5'\n}\nLOG = {\n    'simpleUNET': 'simpleUNET_training.log',\n    'resNETUNET': 'resnetUNET_training.log',\n    'WNET': 'WNET_training.log',\n    'UNETVAE': 'UNETVAE_training.log',\n    'VGG19': 'VGG19_training.log',\n    'InceptionV3': 'InceptionV3_training.log',\n    'InceptionResNetV2': 'InceptionResnetV2_training.log'\n}\n\nSEGMENT_CLASSES = {\n    0: 'NOT tumor',\n    1: 'NECROTIC/CORE',\n    2: 'EDEMA',\n    3: 'ENHANCING'\n}\nSAVED_MODEL = {\n    'WNET': '../input/wnetmodel/WNET.h5',\n    'resNETUNET': '../input/resnet-unet-trained-model/resnetUNET.h5',\n    'simpleUNET': '../input/simpleunet/simpleUNet.h5',\n    'VGG19': '../input/vgg19v2/VGG19.h5',\n    'InceptionV3': '../input/inceptionv3/InceptionV3.h5',\n    'InceptionResNetV2': '../input/inceptionresnetv2/InceptionResNetV2.h5'\n}\nSLICES_VOL = 100\nVOL_START = 22\nIMG_SIZE = 128","metadata":{"_uuid":"57c3d2b9-fec5-4edc-8109-806134128a46","_cell_guid":"1e193d15-7e72-4880-98dd-8780b21e5682","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:42:52.569420Z","iopub.execute_input":"2022-05-23T01:42:52.570215Z","iopub.status.idle":"2022-05-23T01:42:53.269749Z","shell.execute_reply.started":"2022-05-23T01:42:52.570146Z","shell.execute_reply":"2022-05-23T01:42:53.268928Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"e5010854-ab01-4cd2-857c-a8dfcba0ce92","_cell_guid":"c5db215a-0633-4b8a-93da-832cb7f24e77","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def analyse_initial_data():\n    fig, (axis1, axis2, axis3, axis4, axis5) = plt.subplots(1, 5, figsize=(20, 10))\n    slice_w = 25\n    axis1.imshow(VIEW_IMAGE['flair'][:, :,\n               VIEW_IMAGE['flair'].shape[0]//2 - slice_w], cmap='gray')\n    axis1.set_title('Flair')\n    axis2.imshow(\n        VIEW_IMAGE['t1'][:, :, VIEW_IMAGE['t1'].shape[0]//2 - slice_w], cmap='gray')\n    axis2.set_title('T1')\n    axis3.imshow(\n        VIEW_IMAGE['t1ce'][:, :, VIEW_IMAGE['t1ce'].shape[0]//2 - slice_w], cmap='gray')\n    axis3.set_title('T1CE')\n    axis4.imshow(\n        VIEW_IMAGE['t2'][:, :, VIEW_IMAGE['t2'].shape[0]//2 - slice_w], cmap='gray')\n    axis4.set_title('T2')\n    axis5.imshow(VIEW_IMAGE['mask']\n               [:, :, VIEW_IMAGE['mask'].shape[0]//2 - slice_w])\n    axis5.set_title('Mask')\n\n\nanalyse_initial_data()","metadata":{"_uuid":"99ec5907-8cb6-4cba-96e4-19584e898b83","_cell_guid":"2d03f5b3-be0f-4a32-9457-84883b26610e","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:42:53.271135Z","iopub.execute_input":"2022-05-23T01:42:53.271372Z","iopub.status.idle":"2022-05-23T01:42:53.811030Z","shell.execute_reply.started":"2022-05-23T01:42:53.271340Z","shell.execute_reply":"2022-05-23T01:42:53.810325Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"def show_montage():\n    fig, axis = plt.subplots(1, 1, figsize=(15, 15))\n    axis.imshow(rotate(\n        montage(VIEW_IMAGE['flair'][50:-50, :, :]), 90, resize=True), cmap='gray')\n\n\nshow_montage()","metadata":{"_uuid":"6b9bddd5-dbd1-4b9c-995f-8c482c373d81","_cell_guid":"784d78f3-c7b6-4b36-b096-573e4632a918","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:42:53.812353Z","iopub.execute_input":"2022-05-23T01:42:53.812625Z","iopub.status.idle":"2022-05-23T01:42:54.804067Z","shell.execute_reply.started":"2022-05-23T01:42:53.812588Z","shell.execute_reply":"2022-05-23T01:42:54.803312Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"def show_mri_diagram():\n    image = nilearn.image.load_img(LIST_DATA[0])\n    mask = nilearn.image.load_img(LIST_DATA[1])\n    fig, axes = plt.subplots(nrows=4, figsize=(30, 40))\n    nlplt.plot_anat(image, title='ANAT', axes=axes[0])\n    nlplt.plot_epi(image, title='EPI', axes=axes[1])\n    nlplt.plot_img(image, title='IMG', axes=axes[2])\n    nlplt.plot_roi(mask, title='ROI', bg_img=image, axes=axes[3])\n    plt.show()\n\n\nshow_mri_diagram()","metadata":{"_uuid":"dea47444-5d36-41f7-ad34-a1350159fb7c","_cell_guid":"b3fdbbc5-b3c4-45b8-ae49-31456acf1909","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:42:54.805441Z","iopub.execute_input":"2022-05-23T01:42:54.805943Z","iopub.status.idle":"2022-05-23T01:43:03.976536Z","shell.execute_reply.started":"2022-05-23T01:42:54.805901Z","shell.execute_reply":"2022-05-23T01:43:03.975877Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\nclass DataGenerator(keras.utils.all_utils.Sequence):\n    def __init__(self, list_IDs, dimensions=(IMG_SIZE, IMG_SIZE), batch_size=1, n_channels=2, shuffle=True):\n        self.dimensions = dimensions\n        self.batch_size = batch_size\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        Batch_ids = [self.list_IDs[k] for k in indexes]\n        X, y = self.__data_generation(Batch_ids)\n        return X, y\n\n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, Batch_ids):\n        X = np.zeros((self.batch_size*SLICES_VOL,\n                     *self.dimensions, self.n_channels))\n        y = np.zeros((self.batch_size*SLICES_VOL, 240, 240))\n        Y = np.zeros((self.batch_size*SLICES_VOL, *self.dimensions, 4))\n        for c, i in enumerate(Batch_ids):\n            base_path = i\n            data = base_path.split('/')[-1]\n            data_path = f'{i}/{data}_flair.nii'\n            flair = nib.load(data_path).get_fdata()\n            flair = scaler.fit_transform(\n                flair.reshape(-1, flair.shape[-1])).reshape(flair.shape)\n\n            base_path = i\n            data = base_path.split('/')[-1]\n            data_path = f'{i}/{data}_t1ce.nii'\n            ce = nib.load(data_path).get_fdata()\n            ce = scaler.fit_transform(\n                ce.reshape(-1, ce.shape[-1])).reshape(ce.shape)\n\n            base_path = i\n            data = base_path.split('/')[-1]\n            data_path = f'{i}/{data}_seg.nii'\n            seg = nib.load(data_path).get_fdata()\n\n            for j in range(SLICES_VOL):\n                X[j + SLICES_VOL*c, :, :,\n                    0] = cv2.resize(flair[:, :, j+VOL_START], (IMG_SIZE, IMG_SIZE))\n                X[j + SLICES_VOL*c, :, :,\n                    1] = cv2.resize(ce[:, :, j+VOL_START], (IMG_SIZE, IMG_SIZE))\n                y[j + SLICES_VOL*c] = seg[:, :, j+VOL_START]\n\n        y[y == 4] = 3\n        mask = tf.one_hot(y, 4)\n        Y = tf.image.resize(mask, (IMG_SIZE, IMG_SIZE))\n        return X / np.max(X), Y\n\n\ntraining_generator = DataGenerator(TRAIN_LIST)\nval_generator = DataGenerator(VAL_LIST)\ntest_generator = DataGenerator(TEST_LIST)","metadata":{"_uuid":"33d095cf-ea8e-46a5-9c60-b6d1baabafdd","_cell_guid":"6d2b09cc-b969-4748-a5cb-0065445173f9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:43:03.977801Z","iopub.execute_input":"2022-05-23T01:43:03.978260Z","iopub.status.idle":"2022-05-23T01:43:04.002646Z","shell.execute_reply.started":"2022-05-23T01:43:03.978222Z","shell.execute_reply":"2022-05-23T01:43:04.001976Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"def build_unet(inputs, ker_init, dropout):\n    # ------------------------------------------------------\n\n    # Encoding\n    \n    # ------------------------------------------------------\n    convolution = Conv2D(32, 3, activation='relu', padding='same',\n                         kernel_initializer=ker_init)(inputs)\n    convolution = Conv2D(32, 3, activation='relu', padding='same',\n                         kernel_initializer=ker_init)(convolution)\n    pool = MaxPooling2D(pool_size=(2, 2))(convolution)\n    # ------------------------------------------------------\n    convolution1 = Conv2D(64, 3, activation='relu', padding='same',\n                          kernel_initializer=ker_init)(pool)\n    convolution1 = Conv2D(64, 3, activation='relu', padding='same',\n                          kernel_initializer=ker_init)(convolution1)\n    pooling1 = MaxPooling2D(pool_size=(2, 2))(convolution1)\n    # ------------------------------------------------------\n    conv2 = Conv2D(128, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(pooling1)\n    conv2 = Conv2D(128, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(conv2)\n    pooling2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    # ------------------------------------------------------\n    convolution3 = Conv2D(256, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(pooling2)\n    convolution3 = Conv2D(256, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(convolution3)\n    pooling4 = MaxPooling2D(pool_size=(2, 2))(convolution3)\n    # ------------------------------------------------------\n    convolution5 = Conv2D(512, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(pooling4)\n    convolution5 = Conv2D(512, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(convolution5)\n    dropout5 = Dropout(dropout)(convolution5)\n    # ------------------------------------------------------\n\n    # Decoding\n    \n    # ------------------------------------------------------\n    upscaling7 = Conv2D(256, 2, activation='relu', padding='same',\n                 kernel_initializer=ker_init)(UpSampling2D(size=(2, 2))(dropout5))\n    merge_concat7 = concatenate([convolution3, upscaling7], axis=3)\n    convolution7 = Conv2D(256, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(merge_concat7)\n    convolution7 = Conv2D(256, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(convolution7)\n    # ------------------------------------------------------\n    upscaling8 = Conv2D(128, 2, activation='relu', padding='same',\n                 kernel_initializer=ker_init)(UpSampling2D(size=(2, 2))(convolution7))\n    merge_concat8 = concatenate([conv2, upscaling8], axis=3)\n    convolution8 = Conv2D(128, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(merge_concat8)\n    convolution8 = Conv2D(128, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(convolution8)\n    # ------------------------------------------------------\n    upscaling9 = Conv2D(64, 2, activation='relu', padding='same',\n                 kernel_initializer=ker_init)(UpSampling2D(size=(2, 2))(convolution8))\n    merge_concat9 = concatenate([convolution1, upscaling9], axis=3)\n    convolution9 = Conv2D(64, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(merge_concat9)\n    convolution9 = Conv2D(64, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(convolution9)\n    # ------------------------------------------------------\n    upscaling9 = Conv2D(32, 2, activation='relu', padding='same',\n                kernel_initializer=ker_init)(UpSampling2D(size=(2, 2))(convolution9))\n    merge_concat = concatenate([convolution, upscaling9], axis=3)\n    conv = Conv2D(32, 3, activation='relu', padding='same',\n                  kernel_initializer=ker_init)(merge_concat)\n    conv = Conv2D(32, 3, activation='relu', padding='same',\n                  kernel_initializer=ker_init)(conv)\n    convolution10 = Conv2D(4, (1, 1), activation='softmax')(conv)\n\n    return Model(inputs=inputs, outputs=convolution10)","metadata":{"_uuid":"242bdf04-e57f-4766-968d-9f775e2cc925","_cell_guid":"c3bb92c7-3ff2-42a6-8f41-202591f4703e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:43:04.006886Z","iopub.execute_input":"2022-05-23T01:43:04.007286Z","iopub.status.idle":"2022-05-23T01:43:04.027859Z","shell.execute_reply.started":"2022-05-23T01:43:04.007247Z","shell.execute_reply":"2022-05-23T01:43:04.027063Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"def build_unet_vae(input_shape, ker_init, dropout):\n    \n    input_img = Input(shape=input_shape, name='encoder_input')\n    # ------------------------------------------------------\n    \n    # Encoder\n    \n    # ------------------------------------------------------\n\n    convultion1 = Conv2D(32, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(input_img)\n    convultion1 = Conv2D(32, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(convultion1)\n    pooling = MaxPooling2D(pool_size=(2, 2))(convultion1)\n    # ------------------------------------------------------\n    convultion = Conv2D(64, 3, activation='relu', padding='same',\n                  kernel_initializer=ker_init)(pooling)\n    convultion = Conv2D(64, 3, activation='relu', padding='same',\n                  kernel_initializer=ker_init)(convultion)\n    pooling1 = MaxPooling2D(pool_size=(2, 2))(convultion)\n    # ------------------------------------------------------\n    convultion2 = Conv2D(128, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(pooling1)\n    convultion2 = Conv2D(128, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(convultion2)\n    pooling2 = MaxPooling2D(pool_size=(2, 2))(convultion2)\n    # ------------------------------------------------------\n    convultion3 = Conv2D(256, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(pooling2)\n    convultion3 = Conv2D(256, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(convultion3)\n    pooling4 = MaxPooling2D(pool_size=(2, 2))(convultion3)\n    # ------------------------------------------------------\n    convultion5 = Conv2D(512, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(pooling4)\n    convultion5 = Conv2D(512, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(convultion5)\n    dropout5 = Dropout(dropout)(convultion5)\n    # ------------------------------------------------------\n    \n    # VAE\n    latent = 2\n    init_shape_encode = K.int_shape(dropout5)\n    x = Flatten()(dropout5)\n    x = Dense(32, activation='relu')(x)\n    mu = Dense(latent, name='latent_mu')(x)\n    sigma = Dense(latent, name='latent_sigma')(x)\n\n    def sample_z(args):\n        mu, sigma = args\n        eps = K.random_normal(shape=(K.shape(mu)[0], K.int_shape(mu)[1]))\n        return mu + K.exp(sigma / 2) * eps\n\n    z = Lambda(sample_z, output_shape=(latent,), name='z')([mu, sigma])\n    encoder = Model(input_img, [mu, sigma, z], name='encoder')\n    encoder.summary()\n    # ------------------------------------------------------\n    \n    # Decode\n    \n    # ------------------------------------------------------\n    decoder_input = Input(shape=(latent,), name='decoder_input')\n    x = Dense(init_shape_encode[1]*init_shape_encode[2]*init_shape_encode[3],\n              activation='relu')(decoder_input)\n    x = Reshape((init_shape_encode[1], init_shape_encode[2], init_shape_encode[3]))(x)\n    # ------------------------------------------------------\n    upsampling7 = Conv2D(256, 2, activation='relu', padding='same',\n                 kernel_initializer=ker_init)(UpSampling2D(size=(2, 2))(x))\n    convultion7 = Conv2D(256, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(upsampling7)\n    convultion7 = Conv2D(256, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(convultion7)\n    # ------------------------------------------------------\n    upsampling8 = Conv2D(128, 2, activation='relu', padding='same',\n                 kernel_initializer=ker_init)(UpSampling2D(size=(2, 2))(convultion7))\n    convultion8 = Conv2D(128, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(upsampling8)\n    convultion8 = Conv2D(128, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(convultion8)\n    # ------------------------------------------------------\n    upsampling9 = Conv2D(64, 2, activation='relu', padding='same',\n                 kernel_initializer=ker_init)(UpSampling2D(size=(2, 2))(convultion8))\n    convultion9 = Conv2D(64, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(upsampling9)\n    convultion9 = Conv2D(64, 3, activation='relu', padding='same',\n                   kernel_initializer=ker_init)(convultion9)\n    # ------------------------------------------------------\n    upsampling = Conv2D(32, 2, activation='relu', padding='same',\n                kernel_initializer=ker_init)(UpSampling2D(size=(2, 2))(convultion9))\n    convultion = Conv2D(32, 3, activation='relu', padding='same',\n                  kernel_initializer=ker_init)(upsampling)\n    convultion = Conv2D(32, 3, activation='relu', padding='same',\n                  kernel_initializer=ker_init)(convultion)\n    # ------------------------------------------------------\n    convultion10 = Conv2D(4, (1, 1), activation='softmax')(convultion)\n    decoder = Model(decoder_input, convultion10, name='decoder')\n    decoder.summary()\n    z_decoded = decoder(z)\n\n    class VAECustomLayer(keras.layers.Layer):\n        def vae_loss(self, x, z_decoded):\n            x = K.flatten(x)\n            z_decoded = K.flatten(z_decoded)\n            recon_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n            kl_loss = -5e-4 * K.mean(1 + sigma -\n                                     K.square(mu) - K.exp(sigma), axis=-1)\n            return K.mean(recon_loss + kl_loss)\n\n        def call(self, inputs):\n            x = inputs[0]\n            z_decoded = inputs[1]\n            loss = self.vae_loss(x, z_decoded)\n            self.add_loss(loss, inputs=inputs)\n            return x\n\n    y = VAECustomLayer()([input_img, z_decoded])\n    vae = Model(input_img, y, name='vae')\n    return vae","metadata":{"_uuid":"23b33560-331b-494b-884c-3ad70ad30f00","_cell_guid":"34b969b6-bd44-4934-857d-274b7bd72c21","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:43:04.029388Z","iopub.execute_input":"2022-05-23T01:43:04.029768Z","iopub.status.idle":"2022-05-23T01:43:04.060702Z","shell.execute_reply.started":"2022-05-23T01:43:04.029730Z","shell.execute_reply":"2022-05-23T01:43:04.059981Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"def resnet_unet():\n    BACKBONE = 'resnet50'\n    preprocess_input = sm.get_preprocessing(BACKBONE)\n    n_classes = 4     \n    activation = 'softmax'\n    model = sm.Unet(BACKBONE, classes=n_classes, input_shape=(IMG_SIZE, IMG_SIZE, 2), activation=activation, encoder_weights=None, encoder_freeze=True)\n    return model","metadata":{"_uuid":"3cd5f758-31c3-4a0f-9f2c-eeba134b6f39","_cell_guid":"4565c6fb-76f3-4827-953b-6660b52da5d4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:43:04.063508Z","iopub.execute_input":"2022-05-23T01:43:04.064617Z","iopub.status.idle":"2022-05-23T01:43:04.073103Z","shell.execute_reply.started":"2022-05-23T01:43:04.064575Z","shell.execute_reply":"2022-05-23T01:43:04.072401Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"def inceptionv3_unet():\n    BACKBONE = 'inceptionv3'\n    preprocess_input = sm.get_preprocessing(BACKBONE)\n    n_classes = 4\n    activation = 'softmax'\n    model = sm.Unet(BACKBONE, classes=n_classes, input_shape=(\n        IMG_SIZE, IMG_SIZE, 2), activation=activation, encoder_weights=None, encoder_freeze=True)\n    return model","metadata":{"_uuid":"67c9afda-b60e-47c5-87e7-319aaada4f4e","_cell_guid":"06edb6de-5711-490e-be39-27b50e795602","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:43:04.074492Z","iopub.execute_input":"2022-05-23T01:43:04.074683Z","iopub.status.idle":"2022-05-23T01:43:04.086549Z","shell.execute_reply.started":"2022-05-23T01:43:04.074658Z","shell.execute_reply":"2022-05-23T01:43:04.085889Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"def inceptionresnetv2_unet():\n    BACKBONE = 'inceptionresnetv2'\n    preprocess_input = sm.get_preprocessing(BACKBONE)\n    n_classes = 4\n    activation = 'softmax'\n    model = sm.Unet(BACKBONE, classes=n_classes, input_shape=(\n        IMG_SIZE, IMG_SIZE, 2), activation=activation, encoder_weights=None, encoder_freeze=True)\n    return model","metadata":{"_uuid":"1aaea5e9-72aa-4cfb-8f1e-2c23382cdee7","_cell_guid":"185b4a57-f5a7-4d66-bf23-9f658c38d2df","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:43:04.088094Z","iopub.execute_input":"2022-05-23T01:43:04.088648Z","iopub.status.idle":"2022-05-23T01:43:04.095953Z","shell.execute_reply.started":"2022-05-23T01:43:04.088610Z","shell.execute_reply":"2022-05-23T01:43:04.095198Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"def vgg19_unet():\n    BACKBONE = 'vgg19'\n    preprocess_input = sm.get_preprocessing(BACKBONE)\n    n_classes = 4\n    activation = 'softmax'\n    model = sm.Unet(BACKBONE, classes=n_classes, input_shape=(\n        IMG_SIZE, IMG_SIZE, 2), activation=activation, encoder_weights=None, encoder_freeze=True)\n    return model","metadata":{"_uuid":"6b0ba47e-1c4b-43e5-9f25-0832cddaeb89","_cell_guid":"2abf780c-b9c9-4545-a23b-a2a2a6cf833f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:43:04.097613Z","iopub.execute_input":"2022-05-23T01:43:04.098122Z","iopub.status.idle":"2022-05-23T01:43:04.104909Z","shell.execute_reply.started":"2022-05-23T01:43:04.098087Z","shell.execute_reply":"2022-05-23T01:43:04.104146Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"def wnet_architecture():\n    \n    inputs = Input(shape=(128, 128, 2), name='input')\n    # ------------------------------------------------------\n    \n    # Encoder Cycle 1\n    \n    # ------------------------------------------------------\n    \n    cycle_0_convolution1 = Conv2D(\n        32, 3, padding='same', activation='relu')(inputs)\n    cycle_0_normaliaztion1 = BatchNormalization()(cycle_0_convolution1)\n    # ------------------------------------------------------\n    cycle_0_convolution2 = Conv2D(\n        32, 3, padding='same', activation='relu')(cycle_0_normaliaztion1)\n    cycle_0_normalization2 = BatchNormalization()(cycle_0_convolution2)\n    # ------------------------------------------------------\n    cycle_0_pool = MaxPooling2D()(cycle_0_normalization2)\n    # ------------------------------------------------------\n    cycle_1_convolution1 = Conv2D(\n        64, 3, padding='same', activation='relu')(cycle_0_pool)\n    cycle_1_normaliaztion1 = BatchNormalization()(cycle_1_convolution1)\n    # ------------------------------------------------------\n    cycle_1_convolution2 = Conv2D(\n        64, 3, padding='same', activation='relu')(cycle_1_normaliaztion1)\n    cycle_1_normalization2 = BatchNormalization()(cycle_1_convolution2)\n    # ------------------------------------------------------\n    cycle_1_pool = MaxPooling2D()(cycle_1_normalization2)\n    # ------------------------------------------------------\n    cycle_2_convolution1 = Conv2D(\n        128, 3, padding='same', activation='relu')(cycle_1_pool)\n    cycle_2_normaliaztion1 = BatchNormalization()(cycle_2_convolution1)\n    # ------------------------------------------------------\n    cycle_2_convolution2 = Conv2D(\n        128, 3, padding='same', activation='relu')(cycle_2_normaliaztion1)\n    cycle_2_normalization2 = BatchNormalization()(cycle_2_convolution2)\n    # ------------------------------------------------------\n    cycle_2_pool = MaxPooling2D()(cycle_2_normalization2)\n    encoder_dropout_1 = Dropout(0.2)(cycle_2_pool)\n    # ------------------------------------------------------\n    cycle_3_convolution1 = Conv2D(256, 3, padding='same', activation='relu',\n                                  )(encoder_dropout_1)\n    cycle_3_normaliaztion1 = BatchNormalization()(cycle_3_convolution1)\n    # ------------------------------------------------------\n    cycle_3_convolution2 = Conv2D(256, 3, padding='same',\n                                  activation='relu')(cycle_3_normaliaztion1)\n    cycle_3_normalization2 = BatchNormalization()(cycle_3_convolution2)\n    # ------------------------------------------------------\n    cycle_3_pool = MaxPooling2D()(cycle_3_normalization2)\n    # ------------------------------------------------------\n    cycle_4_convolution1 = Conv2D(512, 3, padding='same', activation='relu')(cycle_3_pool)\n    cycle_4_normaliaztion1 = BatchNormalization()(cycle_4_convolution1)\n    # ------------------------------------------------------\n    cycle_4_convolution2 = Conv2D(512, 3, padding='same',\n                                  activation='relu')(cycle_4_normaliaztion1)\n    cycle_4_normalization2 = BatchNormalization()(cycle_4_convolution2)\n    # ------------------------------------------------------\n    cycle_4_pool = MaxPooling2D()(cycle_4_normalization2)\n    # ------------------------------------------------------\n    cycle_5_convolution1 = Conv2D(\n        1024, 3, padding='same', activation='relu')(cycle_4_pool)\n    # ------------------------------------------------------\n    \n    # Decoder Cycle 1\n    \n    # ------------------------------------------------------\n    upscaling_1 = Conv2DTranspose(512, 3, strides=(\n        2, 2), padding='same', activation='relu')(cycle_5_convolution1)\n    merged_cycle_1 = Add()(\n        [cycle_4_normaliaztion1, cycle_4_normalization2, upscaling_1])\n    decode_cycle_1_convolution1 = Conv2D(\n        512, 3, padding='same', activation='relu')(merged_cycle_1)\n    # ------------------------------------------------------\n    upscaling_2 = Conv2DTranspose(256, 3, strides=(\n        2, 2), padding='same', activation='relu')(decode_cycle_1_convolution1)\n    merged_cycle_2 = Add()(\n        [cycle_3_normaliaztion1, cycle_3_normalization2, upscaling_2])\n    decode_cycle_2_convolution1 = Conv2D(\n        256, 3, padding='same', activation='relu')(merged_cycle_2)\n    # ------------------------------------------------------\n    decoder_dropout_1 = Dropout(\n        0.2)(decode_cycle_2_convolution1)\n    # ------------------------------------------------------\n    upscaling_3 = Conv2DTranspose(128, 3, strides=(\n        2, 2), padding='same', activation='relu')(decoder_dropout_1)\n    merged_cycle_3 = Add()(\n        [cycle_2_normaliaztion1, cycle_2_normalization2, upscaling_3])\n    decode_cycle_3_convolution1 = Conv2D(\n        128, 3, padding='same', activation='relu')(merged_cycle_3)\n    # ------------------------------------------------------\n    upscaling_4 = Conv2DTranspose(64, 3, strides=(\n        2, 2), padding='same', activation='relu')(decode_cycle_3_convolution1)\n    merged_cycle_4 = Add()(\n        [cycle_1_normaliaztion1, cycle_1_normaliaztion1, upscaling_4])\n    decode_cycle_4_convolution1 = Conv2D(\n        64, 3, padding='same', activation='relu')(merged_cycle_4)\n    # ------------------------------------------------------\n    upscaling_5 = Conv2DTranspose(32, 3, strides=(\n        2, 2), padding='same', activation='relu')(decode_cycle_4_convolution1)\n    merged_cycle_5 = Add()(\n        [cycle_0_normaliaztion1, cycle_0_normalization2, upscaling_5])\n    decode_cycle_5_convolution1 = Conv2D(\n        32, 3, padding='same', activation='relu')(merged_cycle_5)\n    # ------------------------------------------------------\n    \n    # Encoder Cycle 2\n    \n    # ------------------------------------------------------\n    cycle_6_convolution1 = Conv2D(32, 3, padding='same', activation='relu',\n                                  )(decode_cycle_5_convolution1)\n    cycle_6_normaliaztion1 = BatchNormalization()(cycle_6_convolution1)\n    # ------------------------------------------------------\n    cycle_6_convolution2 = Conv2D(32, 3, padding='same',\n                                  activation='relu')(cycle_6_normaliaztion1)\n    cycle_6_normalization2 = BatchNormalization()(cycle_6_convolution2)\n    # ------------------------------------------------------\n    cycle_6_pool = MaxPooling2D()(cycle_6_normalization2)\n    # ------------------------------------------------------\n    cycle_7_convolution1 = Conv2D(64, 3, padding='same',\n                                  activation='relu')(cycle_6_pool)\n    cycle_7_normaliaztion1 = BatchNormalization()(cycle_7_convolution1)\n    # ------------------------------------------------------\n    cycle_7_convolution2 = Conv2D(64, 3, padding='same',\n                                  activation='relu')(cycle_7_normaliaztion1)\n    cycle_7_normalization2 = BatchNormalization()(cycle_7_convolution2)\n    # ------------------------------------------------------\n    cycle_7_pool = MaxPooling2D()(cycle_7_normalization2)\n    # ------------------------------------------------------\n    cycle_8_convolution1 = Conv2D(128, 3, padding='same',\n                                  activation='relu')(cycle_7_pool)\n    cycle_8_normaliaztion1 = BatchNormalization()(cycle_8_convolution1)\n    # ------------------------------------------------------\n    cycle_8_convolution2 = Conv2D(128, 3, padding='same',\n                                  activation='relu')(cycle_8_normaliaztion1)\n    cycle_8_normalization2 = BatchNormalization()(cycle_8_convolution2)\n    # ------------------------------------------------------\n    cycle_8_pool = MaxPooling2D()(cycle_8_normalization2)\n    # ------------------------------------------------------\n    encoder_dropout_2 = Dropout(0.2)(cycle_8_pool)\n    # ------------------------------------------------------\n    cycle_9_convolution1 = Conv2D(\n        256, 3, padding='same', activation='relu')(encoder_dropout_2)\n    cycle_9_normaliaztion1 = BatchNormalization()(cycle_9_convolution1)\n    # ------------------------------------------------------\n    cycle_9_convolution2 = Conv2D(256, 3, padding='same',\n                                  activation='relu')(cycle_9_normaliaztion1)\n    cycle_9_normalization2 = BatchNormalization()(cycle_9_convolution2)\n    # ------------------------------------------------------\n    cycle_9_pool = MaxPooling2D()(cycle_9_normalization2)\n    # ------------------------------------------------------\n    cycle_10_convolution1 = Conv2D(512, 3, padding='same',\n                                   activation='relu')(cycle_9_pool)\n    cycle_10_normaliaztion1 = BatchNormalization()(cycle_10_convolution1)\n    # ------------------------------------------------------\n    cycle_10_convolution2 = Conv2D(\n        512, 3, padding='same', activation='relu')(cycle_10_normaliaztion1)\n    cycle_10_normalization2 = BatchNormalization()(cycle_10_convolution2)\n    # ------------------------------------------------------\n    cycle_10_pool = MaxPooling2D()(cycle_10_normalization2)\n    # ------------------------------------------------------\n    cycle_11_convolution1 = Conv2D(1024, 3, padding='same',\n                                   activation='relu')(cycle_10_pool)\n    # ------------------------------------------------------\n\n    # Decoder Cycle 2\n\n    # ------------------------------------------------------\n\n    upscaling_6 = Conv2DTranspose(512, 3, strides=(\n        2, 2), padding='same', activation='relu')(cycle_11_convolution1)\n    merged_cycle_6 = Add()(\n        [cycle_10_normaliaztion1, cycle_10_normalization2, upscaling_6])\n    decode_cycle_6_convolution1 = Conv2D(\n        512, 3, padding='same', activation='relu')(merged_cycle_6)\n    # ------------------------------------------------------\n    upscaling_7 = Conv2DTranspose(256, 3, strides=(\n        2, 2), padding='same', activation='relu')(decode_cycle_6_convolution1)\n    merged_cycle_7 = Add()(\n        [cycle_9_normaliaztion1, cycle_9_normalization2, upscaling_7])\n    decode_cycle_7_convolution1 = Conv2D(\n        256, 3, padding='same', activation='relu')(merged_cycle_7)\n    # ------------------------------------------------------\n    decoder_dropout_2 = Dropout(\n        0.2)(decode_cycle_7_convolution1)\n    # ------------------------------------------------------\n    upscaling_8 = Conv2DTranspose(128, 3, strides=(\n        2, 2), padding='same', activation='relu')(decoder_dropout_2)\n    merged_cycle_8 = Add()(\n        [cycle_8_normaliaztion1, cycle_8_normalization2, upscaling_8])\n    decode_cycle_8_convolution1 = Conv2D(\n        128, 3, padding='same', activation='relu')(merged_cycle_8)\n    # ------------------------------------------------------\n    upscaling_9 = Conv2DTranspose(64, 3, strides=(\n        2, 2), padding='same', activation='relu')(decode_cycle_8_convolution1)\n    merged_cycle_9 = Add()(\n        [cycle_7_normaliaztion1, cycle_7_normaliaztion1, upscaling_9])\n    decode_cycle_9_convolution1 = Conv2D(\n        64, 3, padding='same', activation='relu')(merged_cycle_9)\n    # ------------------------------------------------------\n    upscaling_10 = Conv2DTranspose(32, 3, strides=(\n        2, 2), padding='same', activation='relu')(decode_cycle_9_convolution1)\n    merged_cycle_10 = Add()(\n        [cycle_6_normaliaztion1, cycle_6_normalization2, upscaling_10])\n    decode_cycle_10_convolution1 = Conv2D(\n        32, 3, padding='same', activation='relu')(merged_cycle_10)\n    # ------------------------------------------------------\n    pre_output = Conv2D(32, 1, padding='same', activation='relu')(\n        decode_cycle_10_convolution1)\n\n    output = Conv2D(4, 1, padding='same', activation='softmax',\n                    name='output')(pre_output)\n\n    model = Model(inputs=inputs, outputs=output)\n\n    return model","metadata":{"_uuid":"e2fd3f9e-6866-453c-8ad2-7135aa34e036","_cell_guid":"61c7bb1f-a825-4090-9503-5aeb72d63aa3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:43:04.106397Z","iopub.execute_input":"2022-05-23T01:43:04.106938Z","iopub.status.idle":"2022-05-23T01:43:04.145668Z","shell.execute_reply.started":"2022-05-23T01:43:04.106852Z","shell.execute_reply":"2022-05-23T01:43:04.144959Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"optim = tf.optimizers.Adam(0.001)\ntotal_loss = sm.losses.categorical_focal_dice_loss\nmodel = None\nif CURRENT_MODEL == 'simpleUNET':\n    print(\"SIMPLEUNET\")\n    input_layer = Input((IMG_SIZE, IMG_SIZE, 2))\n    model = build_unet(input_layer, 'he_normal', 0.2)\n    model.compile(loss=total_loss, optimizer=optim, metrics=['accuracy', tf.keras.metrics.MeanIoU(\n        num_classes=4), sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)])\nelif CURRENT_MODEL == 'resNETUNET':\n    print(\"RESNETUNET\")\n    model = resnet_unet()\n    model.compile(loss=total_loss, optimizer=optim, metrics=['accuracy', tf.keras.metrics.MeanIoU(\n        num_classes=4), sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)])\nelif CURRENT_MODEL == 'WNET':\n    print(\"WNET\")\n    model = wnet_architecture()\n    model.compile(loss=total_loss, optimizer=optim, metrics=['accuracy', tf.keras.metrics.MeanIoU(\n        num_classes=4), sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)])\nelif CURRENT_MODEL == 'VGG19':\n    print(\"VGG 19\")\n    model = vgg19_unet()\n    adam = tf.keras.optimizers.Adam(lr=0.05, epsilon=0.1)\n    model.compile(loss=total_loss, optimizer=optim, metrics=['accuracy', tf.keras.metrics.MeanIoU(num_classes=4), sm.metrics.IOUScore(\n        threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n    )\nelif CURRENT_MODEL == 'InceptionV3':\n    print(\"InceptionV3\")\n    model = inceptionv3_unet()\n    model.compile(loss=total_loss, optimizer=optim, metrics=['accuracy', tf.keras.metrics.MeanIoU(\n        num_classes=4), sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)])\nelif CURRENT_MODEL == 'InceptionResNetV2':\n    print(\"InceptionResNetV2\")\n    model = inceptionresnetv2_unet()\n    model.compile(loss=total_loss, optimizer=optim, metrics=['accuracy', tf.keras.metrics.MeanIoU(\n        num_classes=4), sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)])\nelif CURRENT_MODEL == 'UNETVAE':\n    model = build_unet_vae((IMG_SIZE, IMG_SIZE, 2), 'he_normal', 0.2)\n    graph = tf.Graph()\n    with graph.as_default():\n        model.compile(loss=total_loss, optimizer=optim, metrics=['accuracy', tf.keras.metrics.MeanIoU(\n            num_classes=4), sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)])","metadata":{"_uuid":"093bbe65-a0f1-4b63-9532-06fa9623b781","_cell_guid":"c64652bc-0b28-4003-a41c-e9d9ed91dddc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:43:04.146979Z","iopub.execute_input":"2022-05-23T01:43:04.147528Z","iopub.status.idle":"2022-05-23T01:43:04.732713Z","shell.execute_reply.started":"2022-05-23T01:43:04.147413Z","shell.execute_reply":"2022-05-23T01:43:04.731994Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"plot_model(model,\n           show_shapes=True,\n           show_dtype=False,\n           show_layer_names=True,\n           rankdir='TB',\n           expand_nested=False,\n           dpi=70)","metadata":{"_uuid":"ebd6d2e6-7323-4c44-8561-108607a3d273","_cell_guid":"e6ce8f8d-2096-45ae-a24a-3b585f7fccb8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:43:04.734564Z","iopub.execute_input":"2022-05-23T01:43:04.734987Z","iopub.status.idle":"2022-05-23T01:43:05.557273Z","shell.execute_reply.started":"2022-05-23T01:43:04.734951Z","shell.execute_reply":"2022-05-23T01:43:05.556346Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"logger = CSVLogger(LOG[CURRENT_MODEL], separator=',', append=False)\ncallbacks = [\n    keras.callbacks.EarlyStopping(monitor='loss', min_delta=0,\n                                  patience=2, verbose=1, mode='auto'),\n    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                                      patience=2, min_lr=0.000001, verbose=1),\n    ModelCheckpoint(filepath=MODEL[CURRENT_MODEL]),\n    logger\n]\nK.clear_session()","metadata":{"_uuid":"a23c29cb-0879-4d8e-92d5-992be6d3f0e4","_cell_guid":"383935d1-4e29-4b67-b93d-fcd29b6181d6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:43:05.559279Z","iopub.execute_input":"2022-05-23T01:43:05.559755Z","iopub.status.idle":"2022-05-23T01:43:05.596598Z","shell.execute_reply.started":"2022-05-23T01:43:05.559718Z","shell.execute_reply":"2022-05-23T01:43:05.595937Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"if MODE == 'TRAIN_FRESH' and model is not None:\n    history = model.fit(training_generator,\n                        epochs=10,\n                        steps_per_epoch=len(TRAIN_LIST),\n                        callbacks=callbacks,\n                        validation_data=val_generator,\n                        verbose=1\n                        )\n    model.save(MODEL[CURRENT_MODEL])\nelse:\n    SAVED_MODEL = {\n        'simpleUNET': '../input/cse676-brats-semantic-segmentation/ccrmodels/simpleUNET/simpleUNET.h5',\n        'resNETUNET': '../input/cse676-brats-semantic-segmentation/ccrmodels/resnetUNET/resnetUNET.h5',\n        'WNET': '../input/cse676-brats-semantic-segmentation/ccrmodels/WNET/WNET.h5',\n        'UNETVAE': 'UNETVAE.h5',\n        'VGG19': '../input/cse676-brats-semantic-segmentation/ccrmodels/VGG19/VGG19.h5',\n        'InceptionV3': '../input/cse676-brats-semantic-segmentation/ccrmodels/InceptionV3/InceptionV3.h5',\n        'InceptionResNetV2': '../input/cse676-brats-semantic-segmentation/ccrmodels/InceptionResNetV2/InceptionResNetV2.h5'\n    }\n    \n    LOG = {\n        'simpleUNET': '../input/cse676-brats-semantic-segmentation/ccrmodels/simpleUNET/simpleUNET_training.log',\n        'resNETUNET': '../input/cse676-brats-semantic-segmentation/ccrmodels/resnetUNET/resnetUNET_training.log',\n        'WNET': '../input/cse676-brats-semantic-segmentation/ccrmodels/WNET/WNET_training.log',\n        'UNETVAE': 'UNETVAE_training.log',\n        'VGG19': '../input/cse676-brats-semantic-segmentation/ccrmodels/VGG19/VGG19_training.log',\n        'InceptionV3': '../input/cse676-brats-semantic-segmentation/ccrmodels/InceptionV3/InceptionV3_training.log',\n        'InceptionResNetV2': '../input/cse676-brats-semantic-segmentation/ccrmodels/InceptionResNetV2/InceptionResNetV2_training.log'\n    }\n    \n    model = keras.models.load_model(SAVED_MODEL[CURRENT_MODEL], custom_objects={\n                                    'accuracy': tf.keras.metrics.MeanIoU(num_classes=4)}, compile=False)","metadata":{"_uuid":"20440985-57e9-4394-8a91-3ec95eb97f10","_cell_guid":"985d954e-889a-4b7c-b54f-76111541b4bc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:43:05.598642Z","iopub.execute_input":"2022-05-23T01:43:05.598914Z","iopub.status.idle":"2022-05-23T01:43:08.445518Z","shell.execute_reply.started":"2022-05-23T01:43:05.598877Z","shell.execute_reply":"2022-05-23T01:43:08.444755Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"#Evaluation Begins","metadata":{"execution":{"iopub.status.busy":"2022-05-23T01:43:08.446997Z","iopub.execute_input":"2022-05-23T01:43:08.447250Z","iopub.status.idle":"2022-05-23T01:43:08.452019Z","shell.execute_reply.started":"2022-05-23T01:43:08.447214Z","shell.execute_reply":"2022-05-23T01:43:08.451188Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"def prepare_data_to_predict(path):\n    base_path = path\n    X = np.empty((SLICES_VOL, IMG_SIZE, IMG_SIZE, 2))\n    data = base_path.split('/')[-1]\n    data_path = f'{path}/{data}_flair.nii'\n    data_path2 = f'{path}/{data}_t1ce.nii'\n    data_path3 = f'{path}/{data}_seg.nii'\n    flair = nib.load(data_path).get_fdata()\n    mask = nib.load(data_path3).get_fdata()\n    ce = nib.load(data_path2).get_fdata()\n    for j in range(SLICES_VOL):\n        X[j, :, :, 0] = cv2.resize(\n            flair[:, :, j + VOL_START], (IMG_SIZE, IMG_SIZE))\n        X[j, :, :, 1] = cv2.resize(\n            ce[:, :, j + VOL_START], (IMG_SIZE, IMG_SIZE))\n    return X, flair, mask","metadata":{"_uuid":"82c384c0-4f87-4d4f-93e1-ffdeaecb8c2a","_cell_guid":"30101188-a8fd-4603-8773-080f6e6b0a82","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:43:08.453670Z","iopub.execute_input":"2022-05-23T01:43:08.453927Z","iopub.status.idle":"2022-05-23T01:43:08.463731Z","shell.execute_reply.started":"2022-05-23T01:43:08.453894Z","shell.execute_reply":"2022-05-23T01:43:08.463037Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"slice = 60\n\n\ndef view_predicted_data(index):\n    data, image, gt = prepare_data_to_predict(TEST_LIST[index])\n    p = model.predict(data/np.max(data), verbose=1)\n    core = p[:, :, :, 1]\n    edema = p[:, :, :, 2]\n    enhancing = p[:, :, :, 3]\n    plt.figure(figsize=(18, 50))\n    f, axis_arr = plt.subplots(1, 6, figsize=(18, 50))\n    for i in range(6):\n        axis_arr[i].imshow(cv2.resize(image[:, :, slice + VOL_START],\n                                      (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n\n    axis_arr[0].imshow(cv2.resize(image[:, :, slice +\n                                        VOL_START], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n    axis_arr[0].title.set_text('Flair')\n    curr_gt = cv2.resize(gt[:, :, slice + VOL_START],\n                         (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n    axis_arr[1].imshow(curr_gt, cmap=\"Reds\", alpha=0.3)\n    axis_arr[1].title.set_text('Ground Truth')\n    axis_arr[2].imshow(p[slice, :, :, 1:4], cmap=\"Reds\", alpha=0.3)\n    axis_arr[2].title.set_text('All Classes')\n    axis_arr[3].imshow(edema[slice, :, :], cmap=\"OrRd\", alpha=0.3)\n    axis_arr[3].title.set_text(f'{SEGMENT_CLASSES[1]} Predicted')\n    axis_arr[4].imshow(core[slice, :, ], cmap=\"OrRd\", alpha=0.3)\n    axis_arr[4].title.set_text(f'{SEGMENT_CLASSES[2]} Predicted')\n    axis_arr[5].imshow(enhancing[slice, :, ], cmap=\"OrRd\", alpha=0.3)\n    axis_arr[5].title.set_text(f'{SEGMENT_CLASSES[3]} Predicted')\n    plt.show()\n\n\nrandom_index = random.sample(range(len(TEST_LIST)), 5)\nfor i in random_index:\n    view_predicted_data(i)","metadata":{"_uuid":"3176fcf1-bdb3-411c-aa85-4d79ae21406d","_cell_guid":"997adcde-1512-44fd-84b5-a12191c93a63","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:43:08.466207Z","iopub.execute_input":"2022-05-23T01:43:08.466482Z","iopub.status.idle":"2022-05-23T01:43:14.491977Z","shell.execute_reply.started":"2022-05-23T01:43:08.466427Z","shell.execute_reply":"2022-05-23T01:43:14.491321Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"path = random_index[-3]\nbase_path = TEST_LIST[path]\ndata, x, gt = prepare_data_to_predict(base_path)\np = model.predict(data/np.max(data), verbose=1)\ncore = p[:,:,:,1]\nedema= p[:,:,:,2]\nenhancing = p[:,:,:,3]\ni = 40 \neval_class = 2 \ngt[gt != eval_class] = 1 \nresized_gt = cv2.resize(gt[:,:,i + VOL_START], (IMG_SIZE, IMG_SIZE))\nplt.figure()\nf, axarr = plt.subplots(1,2) \naxarr[0].imshow(resized_gt, cmap=\"gray\")\naxarr[0].title.set_text('Ground Truth')\naxarr[1].imshow(p[i,:,:,eval_class], cmap=\"gray\")\naxarr[1].title.set_text(f'Predicted Class: {SEGMENT_CLASSES[eval_class]}')\nplt.show()","metadata":{"_uuid":"9a6aa10e-6ea0-4316-9d58-5ce70a898c2f","_cell_guid":"22d623a4-11d4-4790-88ae-9dd32211dd31","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:43:14.493438Z","iopub.execute_input":"2022-05-23T01:43:14.493930Z","iopub.status.idle":"2022-05-23T01:43:15.295533Z","shell.execute_reply.started":"2022-05-23T01:43:14.493891Z","shell.execute_reply":"2022-05-23T01:43:15.294843Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"total_loss = sm.losses.categorical_focal_dice_loss\nmodel.compile(loss=total_loss, optimizer=optim, metrics=['accuracy', tf.keras.metrics.MeanIoU(\n    num_classes=4), sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)])\nprint(\"Evaluation on Test Data\")\nresults = model.evaluate(test_generator, batch_size=100, callbacks=callbacks)\nprint(\"Test Loss, Test Acc:\", results)","metadata":{"_uuid":"785b3924-b053-497f-bc92-818f38a34cff","_cell_guid":"c21321d5-abc4-4e28-a5a7-7151e10e1c8f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:43:15.297061Z","iopub.execute_input":"2022-05-23T01:43:15.297304Z","iopub.status.idle":"2022-05-23T01:43:32.102126Z","shell.execute_reply.started":"2022-05-23T01:43:15.297269Z","shell.execute_reply":"2022-05-23T01:43:32.101248Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"def show_images(epochs, data, label):\n    plt.figure()\n    f, axarr = plt.subplots(1,2, figsize=(12,4)) \n    axarr[0].plot(epochs, data[0], 'y', label=f'Training {label[1]}')\n    axarr[0].plot(epochs, data[1], 'r', label=f'Validation {label[1]}')\n    axarr[0].set_title(f'Training and validation {label[1]}')\n    axarr[0].set_xlabel(label[0])\n    axarr[0].set_ylabel(label[1])\n    axarr[0].legend()\n    \n    axarr[1].plot(epochs, data[2], 'y', label=f'Training {label[2]}')\n    axarr[1].plot(epochs, data[3], 'r', label=f'Validation {label[2]}')\n    axarr[1].set_title(f'Training and validation {label[2]}')\n    axarr[1].set_xlabel(label[0])\n    axarr[1].set_ylabel(label[2])\n    axarr[1].legend()\n    plt.show()","metadata":{"_uuid":"cb33f8a3-8464-48f1-b12b-0af5a244f978","_cell_guid":"a089205d-c6ee-4945-bb6f-7dd30c75e22b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:43:32.103657Z","iopub.execute_input":"2022-05-23T01:43:32.103929Z","iopub.status.idle":"2022-05-23T01:43:32.112559Z","shell.execute_reply.started":"2022-05-23T01:43:32.103893Z","shell.execute_reply":"2022-05-23T01:43:32.111792Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"history = pd.read_csv(LOG[CURRENT_MODEL], sep=',', engine='python')\nn = history['epoch']\nhist = history\nacc=hist['accuracy']\nval_acc=hist['val_accuracy']\nloss=hist['loss']\nval_loss=hist['val_loss']\nshow_images(n, [acc, val_acc, loss, val_loss], ['Epochs', 'Accuracy', 'Loss'])","metadata":{"_uuid":"6808a6aa-aaa6-4aac-8f99-12bed7b87d9b","_cell_guid":"343d4170-7059-4b6d-aac6-77329c8a36c7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-23T01:43:32.113877Z","iopub.execute_input":"2022-05-23T01:43:32.114436Z","iopub.status.idle":"2022-05-23T01:43:32.443961Z","shell.execute_reply.started":"2022-05-23T01:43:32.114395Z","shell.execute_reply":"2022-05-23T01:43:32.443293Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}